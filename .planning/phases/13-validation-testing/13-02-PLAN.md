---
phase: 13-validation-testing
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - tests/performance/state-reconstruction.bench.js
  - tests/performance/orchestrator.bench.js
  - tests/performance/validation.bench.js
  - tests/performance/benchmark-runner.js
autonomous: true

must_haves:
  truths:
    - "State reconstruction completes in <100ms for 100-delta chains"
    - "Orchestrator context assembly completes in <1s for 10-character scenes"
    - "Full validation completes in <30s for realistic database"
  artifacts:
    - path: "tests/performance/state-reconstruction.bench.js"
      provides: "State reconstruction benchmarks"
      min_lines: 80
    - path: "tests/performance/orchestrator.bench.js"
      provides: "Orchestrator context assembly benchmarks"
      min_lines: 100
    - path: "tests/performance/validation.bench.js"
      provides: "Validation performance benchmarks"
      min_lines: 60
    - path: "tests/performance/benchmark-runner.js"
      provides: "Unified benchmark execution"
      min_lines: 100
  key_links:
    - from: "benchmark-runner.js"
      to: "all bench.js files"
      via: "imports and executes all benchmarks"
      pattern: "require.*bench"
    - from: "benchmarks"
      to: "performance-report.json"
      via: "writes results to JSON report"
      pattern: "writeFileSync.*performance-report"
---

<objective>
Create comprehensive performance benchmark suite validating performance targets.

Purpose: Verify TripleThink v4.1 meets performance requirements for production use. Benchmarks establish baseline metrics for state reconstruction, orchestrator operations, and validation execution.

Output: Performance benchmark suite with automated reporting proving all targets met.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Codebase context
@.planning/codebase/TESTING.md
@.planning/codebase/ARCHITECTURE.md

# Performance targets reference
@.planning/REQUIREMENTS.md

# Existing performance report (template)
@performance-report.json

# Modules to benchmark
@db/state-reconstruction.js
@api/services/orchestrator.js
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create state reconstruction benchmarks</name>
  <files>tests/performance/state-reconstruction.bench.js</files>
  <action>
Create benchmarks verifying state reconstruction <100ms for 100-delta chains:

**Test scenarios:**
1. Best case: Recent snapshot (5 deltas)
2. Typical case: Mid-range (50 deltas)
3. Target case: 100 deltas from snapshot
4. Worst case: 200 deltas (should still be reasonable)

**Implementation:**
```javascript
const Database = require('better-sqlite3');
const path = require('path');
const fs = require('fs');
const StateReconstruction = require('../../db/state-reconstruction');

async function benchmarkStateReconstruction() {
  const results = [];

  // Setup: Create test database with snapshots and deltas
  const db = new Database(':memory:');
  // ... load schema, create test data

  const stateRecon = new StateReconstruction(db);

  // Benchmark 1: 5-delta chain (best case)
  const start1 = performance.now();
  for (let i = 0; i < 100; i++) {
    await stateRecon.getAssetStateAtEvent('asset-test-1', 'evt-target-5');
  }
  const duration1 = (performance.now() - start1) / 100;

  results.push({
    scenario: '5-delta chain (best case)',
    iterations: 100,
    avg_duration_ms: duration1,
    target_ms: 100,
    passed: duration1 < 100
  });

  // Benchmark 2: 50-delta chain (typical)
  const start2 = performance.now();
  for (let i = 0; i < 50; i++) {
    await stateRecon.getAssetStateAtEvent('asset-test-2', 'evt-target-50');
  }
  const duration2 = (performance.now() - start2) / 50;

  results.push({
    scenario: '50-delta chain (typical)',
    iterations: 50,
    avg_duration_ms: duration2,
    target_ms: 100,
    passed: duration2 < 100
  });

  // Benchmark 3: 100-delta chain (target case)
  const start3 = performance.now();
  for (let i = 0; i < 20; i++) {
    await stateRecon.getAssetStateAtEvent('asset-test-3', 'evt-target-100');
  }
  const duration3 = (performance.now() - start3) / 20;

  results.push({
    scenario: '100-delta chain (target)',
    iterations: 20,
    avg_duration_ms: duration3,
    target_ms: 100,
    passed: duration3 < 100
  });

  // Benchmark 4: 200-delta chain (worst case - should warn)
  const start4 = performance.now();
  for (let i = 0; i < 10; i++) {
    await stateRecon.getAssetStateAtEvent('asset-test-4', 'evt-target-200');
  }
  const duration4 = (performance.now() - start4) / 10;

  results.push({
    scenario: '200-delta chain (worst case)',
    iterations: 10,
    avg_duration_ms: duration4,
    target_ms: 200, // Relaxed target
    passed: duration4 < 200,
    warning: duration4 > 150 ? 'Consider creating snapshot' : null
  });

  db.close();

  return {
    category: 'State Reconstruction',
    target_ms: 100,
    all_passed: results.every(r => r.passed),
    results
  };
}

module.exports = benchmarkStateReconstruction;
```

**Test data generation:**
- Create asset with snapshot at event 0
- Create delta chain of 200 events
- Target events at positions 5, 50, 100, 200
- Measure time to reconstruct state
  </action>
  <verify>node tests/performance/state-reconstruction.bench.js</verify>
  <done>State reconstruction benchmarks pass with <100ms for 100-delta chains</done>
</task>

<task type="auto">
  <name>Task 2: Create orchestrator context assembly benchmarks</name>
  <files>tests/performance/orchestrator.bench.js</files>
  <action>
Create benchmarks verifying orchestrator context assembly <1s for 10-character scenes:

**Test scenarios:**
1. Simple scene: 2 characters, no conflicts
2. Typical scene: 5 characters, 2 conflicts
3. Target scene: 10 characters, 5 conflicts, 3 arcs
4. Complex scene: 15 characters (should warn if >1s)

**Implementation:**
```javascript
const Database = require('better-sqlite3');
const Orchestrator = require('../../api/services/orchestrator');

async function benchmarkOrchestrator() {
  const results = [];
  const db = new Database(':memory:');
  // ... load schema, create test fiction with characters, events, logic layer

  const orchestrator = new Orchestrator(db);

  // Benchmark 1: 2-character scene
  const start1 = performance.now();
  for (let i = 0; i < 50; i++) {
    const context = await orchestrator.assembleContext({
      fictionId: 'fic-test',
      sceneId: 'scene-simple',
      presentEntityIds: ['char-1', 'char-2']
    });
  }
  const duration1 = (performance.now() - start1) / 50;

  results.push({
    scenario: '2-character scene (simple)',
    iterations: 50,
    avg_duration_ms: duration1,
    target_ms: 1000,
    passed: duration1 < 1000
  });

  // Benchmark 2: 5-character scene
  const start2 = performance.now();
  for (let i = 0; i < 30; i++) {
    const context = await orchestrator.assembleContext({
      fictionId: 'fic-test',
      sceneId: 'scene-typical',
      presentEntityIds: ['char-1', 'char-2', 'char-3', 'char-4', 'char-5'],
      activeConflictIds: ['conf-1', 'conf-2']
    });
  }
  const duration2 = (performance.now() - start2) / 30;

  results.push({
    scenario: '5-character scene with conflicts (typical)',
    iterations: 30,
    avg_duration_ms: duration2,
    target_ms: 1000,
    passed: duration2 < 1000
  });

  // Benchmark 3: 10-character scene (target case)
  const start3 = performance.now();
  for (let i = 0; i < 20; i++) {
    const context = await orchestrator.assembleContext({
      fictionId: 'fic-test',
      sceneId: 'scene-target',
      presentEntityIds: Array.from({length: 10}, (_, i) => `char-${i+1}`),
      activeConflictIds: ['conf-1', 'conf-2', 'conf-3', 'conf-4', 'conf-5']
    });
  }
  const duration3 = (performance.now() - start3) / 20;

  results.push({
    scenario: '10-character scene with logic layer (target)',
    iterations: 20,
    avg_duration_ms: duration3,
    target_ms: 1000,
    passed: duration3 < 1000,
    details: {
      characters: 10,
      conflicts: 5,
      arcs_loaded: 10,
      epistemic_states_loaded: 10
    }
  });

  // Benchmark 4: 15-character scene (stress test)
  const start4 = performance.now();
  for (let i = 0; i < 10; i++) {
    const context = await orchestrator.assembleContext({
      fictionId: 'fic-test',
      sceneId: 'scene-complex',
      presentEntityIds: Array.from({length: 15}, (_, i) => `char-${i+1}`)
    });
  }
  const duration4 = (performance.now() - start4) / 10;

  results.push({
    scenario: '15-character scene (stress test)',
    iterations: 10,
    avg_duration_ms: duration4,
    target_ms: 2000, // Relaxed target
    passed: duration4 < 2000,
    warning: duration4 > 1000 ? 'Large scene, consider splitting' : null
  });

  db.close();

  return {
    category: 'Orchestrator Context Assembly',
    target_ms: 1000,
    all_passed: results.every(r => r.passed),
    results
  };
}

module.exports = benchmarkOrchestrator;
```

**Context packet verification:**
- Verify all character epistemic states loaded
- Verify all arcs for present characters included
- Verify all active conflicts included
- Measure time from call to complete packet returned
  </action>
  <verify>node tests/performance/orchestrator.bench.js</verify>
  <done>Orchestrator benchmarks pass with <1s for 10-character scenes</done>
</task>

<task type="auto">
  <name>Task 3: Create validation and unified benchmark runner</name>
  <files>tests/performance/validation.bench.js, tests/performance/benchmark-runner.js</files>
  <action>
Create validation performance benchmark and unified runner:

**validation.bench.js:**
```javascript
const Database = require('better-sqlite3');
const TripleThinkValidator = require('../../api/services/validator');

async function benchmarkValidation() {
  const results = [];
  const db = new Database(':memory:');
  // ... load schema, create realistic test database (2000+ entities)

  const validator = new TripleThinkValidator(db);

  // Benchmark full validation
  const start = performance.now();
  const report = await validator.validateDatabase();
  const duration = performance.now() - start;

  results.push({
    scenario: 'Full database validation (2000+ entities)',
    duration_ms: duration,
    target_ms: 30000,
    passed: duration < 30000,
    details: {
      rules_executed: report.summary.total_rules,
      categories: Object.keys(report.categories).length,
      entities_validated: 2000,
      errors_found: report.summary.failed,
      warnings_found: report.summary.warnings
    }
  });

  // Benchmark by category
  for (const category of Object.keys(validator.rules)) {
    const start = performance.now();
    await validator.runCategoryRules(category, validator.rules[category]);
    const duration = performance.now() - start;

    results.push({
      scenario: `Category: ${category}`,
      duration_ms: duration,
      rule_count: validator.rules[category].length
    });
  }

  db.close();

  return {
    category: 'Validation',
    target_ms: 30000,
    all_passed: results[0].passed,
    results
  };
}

module.exports = benchmarkValidation;
```

**benchmark-runner.js:**
```javascript
const fs = require('fs');
const path = require('path');

const benchmarkStateReconstruction = require('./state-reconstruction.bench');
const benchmarkOrchestrator = require('./orchestrator.bench');
const benchmarkValidation = require('./validation.bench');

async function runAllBenchmarks() {
  console.log('ðŸš€ TripleThink v4.1 Performance Benchmarks\n');

  const report = {
    timestamp: new Date().toISOString(),
    title: 'TripleThink v4.1 Performance Benchmarks',
    summary: {
      all_targets_met: true,
      total_operations: 3,
      passed: 0,
      failed: 0
    },
    performance_targets: {
      state_reconstruction_ms: 100,
      orchestrator_operation_ms: 1000,
      full_validation_ms: 30000
    },
    results: []
  };

  // Run state reconstruction benchmarks
  console.log('Running state reconstruction benchmarks...');
  const stateResults = await benchmarkStateReconstruction();
  report.results.push(stateResults);
  if (stateResults.all_passed) report.summary.passed++;
  else report.summary.failed++;

  // Run orchestrator benchmarks
  console.log('Running orchestrator benchmarks...');
  const orchResults = await benchmarkOrchestrator();
  report.results.push(orchResults);
  if (orchResults.all_passed) report.summary.passed++;
  else report.summary.failed++;

  // Run validation benchmarks
  console.log('Running validation benchmarks...');
  const validResults = await benchmarkValidation();
  report.results.push(validResults);
  if (validResults.all_passed) report.summary.passed++;
  else report.summary.failed++;

  report.summary.all_targets_met = report.summary.failed === 0;

  // Write report
  const reportPath = path.join(__dirname, '../../performance-report.json');
  fs.writeFileSync(reportPath, JSON.stringify(report, null, 2));

  console.log(`\nâœ… Benchmarks complete. Report: ${reportPath}`);
  console.log(`Targets met: ${report.summary.passed}/${report.summary.total_operations}`);

  if (!report.summary.all_targets_met) {
    console.error('âŒ Some performance targets not met!');
    process.exit(1);
  }
}

runAllBenchmarks().catch(err => {
  console.error('Benchmark error:', err);
  process.exit(1);
});
```

Run with: `node tests/performance/benchmark-runner.js`
  </action>
  <verify>node tests/performance/benchmark-runner.js && cat performance-report.json | jq '.summary.all_targets_met'</verify>
  <done>All performance benchmarks pass and report generated successfully</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] Run benchmark-runner.js and verify all targets met
- [ ] Check performance-report.json generated with all_targets_met: true
- [ ] Verify state reconstruction <100ms for 100-delta chains
- [ ] Verify orchestrator <1s for 10-character scenes
- [ ] Verify full validation <30s
</verification>

<success_criteria>
- All performance benchmarks implemented and passing
- performance-report.json generated with comprehensive results
- State reconstruction meets <100ms target
- Orchestrator context assembly meets <1s target
- Full validation meets <30s target
- Benchmark runner exits with status 0 (all tests passed)
</success_criteria>

<output>
After completion, create `.planning/phases/13-validation-testing/13-02-SUMMARY.md`
</output>
